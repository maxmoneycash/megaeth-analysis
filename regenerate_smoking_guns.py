#!/usr/bin/env python3
"""Regenerate all smoking gun CSVs with fresh data from MegaETH dashboard."""

import json
import sys
from datetime import datetime
import statistics
import csv

# Load the anomaly report
with open("./ralphy-output/historical/anomaly-report.json") as f:
    data = json.load(f)

# Get raw historical data
gas_data = data["rawHistoricalData"]["gas7d"]
tps_data = data["rawHistoricalData"]["tps7d"]
interval_data = data["rawHistoricalData"]["interval7d"]

# Convert to usable format
gas_values = [(p["timestamp"], p["value"] / 1_000_000) for p in gas_data]
tps_values = [(p["timestamp"], p["value"] / 1000) for p in tps_data]
interval_values = [(p["timestamp"], p["value"]) for p in interval_data]

print(f"Loaded {len(gas_values)} data points")
print(f"Date range: {datetime.fromtimestamp(gas_values[0][0]).isoformat()}Z to {datetime.fromtimestamp(gas_values[-1][0]).isoformat()}Z")

# Calculate stats
gas_mean = statistics.mean([v for _, v in gas_values])
gas_std = statistics.stdev([v for _, v in gas_values])
tps_mean = statistics.mean([v for _, v in tps_values])
tps_std = statistics.stdev([v for _, v in tps_values])
interval_mean = statistics.mean([v for _, v in interval_values])
interval_std = statistics.stdev([v for _, v in interval_values])

print(f"\nStats:")
print(f"  Gas: mean={gas_mean:.1f} MGas/s, std={gas_std:.1f}")
print(f"  TPS: mean={tps_mean:.2f}K, std={tps_std:.2f}")
print(f"  Block interval: mean={interval_mean:.3f}ms, std={interval_std:.3f}")

# 1. SMOKING GUN 3: Worst moments for each metric
print("\n1. Creating smoking_gun_3_worst_moments.csv...")
worst_moments = []

# Gas drops (lowest values)
gas_sorted = sorted(gas_values, key=lambda x: x[1])[:10]
for ts, v in gas_sorted:
    dt = datetime.fromtimestamp(ts).isoformat() + "Z"
    pct = (v / 2048) * 100
    verdict = "THROUGHPUT CRASHED" if v < 1800 else "THROUGHPUT DEGRADED"
    worst_moments.append({
        "rank": len([x for x in worst_moments if x["metric"] == "gas"]) + 1,
        "metric": "gas",
        "timestamp": dt,
        "value": round(v, 2),
        "unit": "MGas/s",
        "normal_value": 2048,
        "pct_of_normal": f"{pct:.1f}%",
        "verdict": verdict
    })

# TPS drops
tps_sorted = sorted(tps_values, key=lambda x: x[1])[:10]
for ts, v in tps_sorted:
    dt = datetime.fromtimestamp(ts).isoformat() + "Z"
    pct = (v / 21.0) * 100
    verdict = "TPS CRASHED" if v < 18 else "TPS DEGRADED"
    worst_moments.append({
        "rank": len([x for x in worst_moments if x["metric"] == "tps"]) + 1,
        "metric": "tps",
        "timestamp": dt,
        "value": round(v, 2),
        "unit": "K TPS",
        "normal_value": 21.0,
        "pct_of_normal": f"{pct:.1f}%",
        "verdict": verdict
    })

# Block interval spikes (highest values)
interval_sorted = sorted(interval_values, key=lambda x: -x[1])[:10]
for ts, v in interval_sorted:
    dt = datetime.fromtimestamp(ts).isoformat() + "Z"
    pct = (v / 9.32) * 100
    verdict = "BLOCKS SLOWED" if v > 9.5 else "BLOCKS SLIGHTLY SLOW"
    worst_moments.append({
        "rank": len([x for x in worst_moments if x["metric"] == "block_interval"]) + 1,
        "metric": "block_interval",
        "timestamp": dt,
        "value": round(v, 3),
        "unit": "ms",
        "normal_value": 9.32,
        "pct_of_normal": f"{pct:.1f}%",
        "verdict": verdict
    })

with open("smoking_gun_3_worst_moments.csv", "w", newline="") as f:
    writer = csv.DictWriter(f, fieldnames=["rank", "metric", "timestamp", "value", "unit", "normal_value", "pct_of_normal", "verdict"])
    writer.writeheader()
    writer.writerows(worst_moments)
print(f"   Created with {len(worst_moments)} rows")

# 2. SMOKING GUN 2: Correlated drops (when multiple metrics fail together)
print("\n2. Creating smoking_gun_2_correlated_drops.csv...")
correlated = []
for i in range(len(gas_values)):
    ts = gas_values[i][0]
    gas_v = gas_values[i][1]
    tps_v = tps_values[i][1]
    interval_v = interval_values[i][1]

    gas_bad = gas_v < gas_mean - 2*gas_std
    tps_bad = tps_v < tps_mean - 2*tps_std
    interval_bad = interval_v > interval_mean + 2*interval_std

    degraded_count = sum([gas_bad, tps_bad, interval_bad])

    if degraded_count >= 2:
        dt = datetime.fromtimestamp(ts)
        window_start = dt.replace(second=0, microsecond=0)
        window_start = window_start.replace(minute=(dt.minute // 10) * 10)

        correlated.append({
            "window_start": window_start.isoformat() + "Z",
            "timestamp_unix": ts,
            "degraded_metrics": degraded_count,
            "gas_drop": round(gas_v, 2),
            "tps_drop": round(tps_v, 2),
            "block_time_spike": round(interval_v, 3),
            "severity": "CRITICAL" if degraded_count == 3 else "WARNING"
        })

# Dedupe by window
seen_windows = set()
unique_correlated = []
for c in correlated:
    if c["window_start"] not in seen_windows:
        seen_windows.add(c["window_start"])
        unique_correlated.append(c)

with open("smoking_gun_2_correlated_drops.csv", "w", newline="") as f:
    writer = csv.DictWriter(f, fieldnames=["window_start", "timestamp_unix", "degraded_metrics", "gas_drop", "tps_drop", "block_time_spike", "severity"])
    writer.writeheader()
    writer.writerows(unique_correlated)
print(f"   Created with {len(unique_correlated)} rows")

# 3. SMOKING GUN 4: Anomaly duration periods
print("\n3. Creating smoking_gun_4_anomaly_duration.csv...")
anomaly_periods = []
current_period = None

for i in range(len(gas_values)):
    ts = gas_values[i][0]
    gas_v = gas_values[i][1]
    tps_v = tps_values[i][1]
    interval_v = interval_values[i][1]

    is_anomaly = (gas_v < gas_mean - 2*gas_std or
                  tps_v < tps_mean - 2*tps_std or
                  interval_v > interval_mean + 2*interval_std)

    if is_anomaly:
        types = []
        if gas_v < gas_mean - 2*gas_std:
            types.append("GAS_DROP")
        if tps_v < tps_mean - 2*tps_std:
            types.append("TPS_DROP")
        if interval_v > interval_mean + 2*interval_std:
            types.append("BLOCK_TIME_SPIKE")

        if current_period is None:
            current_period = {
                "start_ts": ts,
                "end_ts": ts,
                "count": 1,
                "types": set(types)
            }
        else:
            # Check if within 30 min of last anomaly
            if ts - current_period["end_ts"] < 1800:
                current_period["end_ts"] = ts
                current_period["count"] += 1
                current_period["types"].update(types)
            else:
                anomaly_periods.append(current_period)
                current_period = {
                    "start_ts": ts,
                    "end_ts": ts,
                    "count": 1,
                    "types": set(types)
                }
    else:
        if current_period is not None:
            anomaly_periods.append(current_period)
            current_period = None

if current_period:
    anomaly_periods.append(current_period)

duration_rows = []
for p in anomaly_periods:
    start_dt = datetime.fromtimestamp(p["start_ts"]).isoformat() + "Z"
    end_dt = datetime.fromtimestamp(p["end_ts"]).isoformat() + "Z"
    duration_min = (p["end_ts"] - p["start_ts"]) / 60
    severity = "CRITICAL" if p["count"] > 10 or duration_min > 60 else "WARNING"

    duration_rows.append({
        "start_time": start_dt,
        "end_time": end_dt,
        "duration_minutes": round(duration_min, 1),
        "anomaly_count": p["count"],
        "anomaly_types": ", ".join(sorted(p["types"])),
        "severity": severity
    })

with open("smoking_gun_4_anomaly_duration.csv", "w", newline="") as f:
    writer = csv.DictWriter(f, fieldnames=["start_time", "end_time", "duration_minutes", "anomaly_count", "anomaly_types", "severity"])
    writer.writeheader()
    writer.writerows(sorted(duration_rows, key=lambda x: -x["duration_minutes"]))
print(f"   Created with {len(duration_rows)} rows")

# 4. SMOKING GUN 5: Claims vs Reality (updated)
print("\n4. Creating smoking_gun_5_claims_vs_reality.csv...")
min_tps = min(tps_values, key=lambda x: x[1])[1]
min_gas = min(gas_values, key=lambda x: x[1])[1]
max_interval = max(interval_values, key=lambda x: x[1])[1]

claims = [
    {"metric": "TPS Capacity", "megaeth_claim": "100,000+ TPS", "measured_avg": "21.0K", "measured_worst": f"{min_tps:.1f}K", "times_worse": f"{100000/21000:.1f}x below claim", "verdict": "INFLATED (99% synthetic)"},
    {"metric": "Block Time", "megaeth_claim": "10ms", "measured_avg": f"{interval_mean:.2f}ms", "measured_worst": f"{max_interval:.2f}ms", "times_worse": "Within spec", "verdict": "PASSED"},
    {"metric": "Gas Throughput", "megaeth_claim": "~2000 MGas/s", "measured_avg": f"{gas_mean:.0f} MGas/s", "measured_worst": f"{min_gas:.0f} MGas/s", "times_worse": f"{((2048-min_gas)/2048)*100:.0f}% drop", "verdict": "UNSTABLE"},
    {"metric": "Traffic Authenticity", "megaeth_claim": "Real user transactions", "measured_avg": "~99% synthetic", "measured_worst": "71% dust spam + 29% fake DEX", "times_worse": "N/A", "verdict": "FAKE TRAFFIC"},
]

with open("smoking_gun_5_claims_vs_reality.csv", "w", newline="") as f:
    writer = csv.DictWriter(f, fieldnames=["metric", "megaeth_claim", "measured_avg", "measured_worst", "times_worse", "verdict"])
    writer.writeheader()
    writer.writerows(claims)
print(f"   Created with {len(claims)} rows")

# 5. SMOKING GUN 6: Hourly patterns
print("\n5. Creating smoking_gun_6_hourly_patterns.csv...")
hourly_data = {}
for i in range(len(gas_values)):
    ts = gas_values[i][0]
    hour = datetime.fromtimestamp(ts).hour
    if hour not in hourly_data:
        hourly_data[hour] = {"gas": [], "tps": [], "interval": []}
    hourly_data[hour]["gas"].append(gas_values[i][1])
    hourly_data[hour]["tps"].append(tps_values[i][1])
    hourly_data[hour]["interval"].append(interval_values[i][1])

hourly_rows = []
for hour in range(24):
    if hour in hourly_data:
        d = hourly_data[hour]
        hourly_rows.append({
            "hour_utc": f"{hour:02d}:00",
            "avg_gas_mgas": round(statistics.mean(d["gas"]), 1),
            "min_gas_mgas": round(min(d["gas"]), 1),
            "avg_tps_k": round(statistics.mean(d["tps"]), 2),
            "min_tps_k": round(min(d["tps"]), 2),
            "sample_count": len(d["gas"])
        })

with open("smoking_gun_6_hourly_patterns.csv", "w", newline="") as f:
    writer = csv.DictWriter(f, fieldnames=["hour_utc", "avg_gas_mgas", "min_gas_mgas", "avg_tps_k", "min_tps_k", "sample_count"])
    writer.writeheader()
    writer.writerows(hourly_rows)
print(f"   Created with {len(hourly_rows)} rows")

print("\nâœ… All smoking gun CSVs regenerated with fresh data!")
